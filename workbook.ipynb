{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eba6900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import wordnet, words, stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger_eng', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('words', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "694759cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants and Patterns\n",
    "\n",
    "# Regex for matching roman numerals\n",
    "ROMAN_NUMERAL_PATTERN = re.compile(\n",
    "    r'^(M{0,3})(CM|CD|D?C{0,3})'\n",
    "    r'(XC|XL|L?X{0,3})(IX|IV|V?I{0,3})$', re.IGNORECASE\n",
    ")\n",
    "\n",
    "# Spelled-out numbers and their variations\n",
    "SPELLED_NUMBERS = [\n",
    "    \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\",\n",
    "    \"eleven\", \"twelve\", \"thirteen\", \"fourteen\", \"fifteen\", \"sixteen\", \"seventeen\", \"eighteen\", \"nineteen\", \"twenty\",\n",
    "    \"thirty\", \"forty\", \"fifty\", \"sixty\", \"seventy\", \"eighty\", \"ninety\",\n",
    "    \"hundred\", \"thousand\", \"million\", \"billion\"\n",
    "]\n",
    "\n",
    "SPELLED_NUMBERS += ['first', 'second', 'third', 'fourth', 'fifth', 'sixth', 'seventh', 'eighth', 'ninth', 'tenth']\n",
    "SPELLED_NUMBERS += ['eleventh', 'twelfth', 'thirteenth', 'fourteenth', 'fifteenth', 'sixteenth', 'seventeenth', 'eighteenth', 'nineteenth']\n",
    "SPELLED_NUMBERS += ['twentieth', 'thirtieth', 'fortieth', 'fiftieth', 'sixtieth', 'seventieth', 'eightieth', 'ninetieth']\n",
    "SPELLED_NUMBERS += ['hundredth', 'thousandth', 'millionth', 'billionth']\n",
    "\n",
    "SPELLED_NUMBERS += ['twice', 'thrice', 'once']\n",
    "SPELLED_NUMBERS += ['single', 'double', 'triple', 'quadruple', 'quintuple', 'sextuple', 'septuple', 'octuple', 'nonuple', 'decuple']\n",
    "SPELLED_NUMBERS += ['dozen', 'fortnight', 'score', 'century', 'millennium']\n",
    "\n",
    "# Set of spelled-out numbers for quick lookup\n",
    "SPELLED_NUMBERS_SET = set(SPELLED_NUMBERS)\n",
    "\n",
    "# Set of stopwords from NLTK\n",
    "STOPWORDS = set(stopwords.words(\"english\"))\n",
    "\n",
    "# English vocabulary set for filtering unusual proper nouns\n",
    "ENGLISH_VOCAB = set(w.lower() for w in words.words())\n",
    "\n",
    "# Number of samples to show in the display function\n",
    "NUMBER_OF_SAMPLES_TO_SHOW = 3\n",
    "\n",
    "# Threshold for low frequency words, used in filtering unusual proper nouns\n",
    "LOW_FREQUENCY_THRESHOLD = 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9075e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# File functions\n",
    "# Load JSON data from a file\n",
    "def load_json_data(file_path):\n",
    "    '''\n",
    "    Loads JSON data from the specified file path into a pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the JSON file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing the loaded JSON data, or None if loading fails.\n",
    "    '''\n",
    "    try:\n",
    "        data = pd.read_json(file_path, orient=\"records\", lines=False)\n",
    "        return data\n",
    "    except ValueError as e:\n",
    "        print(f\"Error loading JSON data: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def strip_html_tags(text):\n",
    "    \"\"\"\n",
    "    Strips HTML tags from a string.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input string potentially containing HTML tags.\n",
    "\n",
    "    Returns:\n",
    "        str: The string with HTML tags removed.\n",
    "    \"\"\"\n",
    "    # Regex to find any HTML tag: < followed by any characters, then >\n",
    "    # The '?' makes it non-greedy, matching the shortest possible string.\n",
    "    # The '|' handles self-closing tags like <br/> or <img src=\"...\">\n",
    "    # and also comments (though less common in user-generated text usually)\n",
    "    clean = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
    "    return re.sub(clean, '', text)\n",
    "\n",
    "def display_sample_questions(df, lookup_column = None, extra_details_col = None, sample_count = 3):\n",
    "    \"\"\"\n",
    "    Displays a sample of questions from the DataFrame where the specified lookup_column is True.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing the questions.\n",
    "        lookup_column (str, optional): The column name to use as a boolean filter for selecting rows.\n",
    "        sample_count (int, optional): The number of sample questions to display. Default is 3.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If lookup_column is None or empty.\n",
    "\n",
    "    Prints:\n",
    "        The index, category, and question text for each sampled row.\n",
    "    \"\"\"\n",
    "    if lookup_column is None:\n",
    "        raise ValueError(\"lookup_column argument cannot be empty or None\")\n",
    "    \n",
    "    if df is None or lookup_column not in df.columns:\n",
    "        raise ValueError(f\"DataFrame is None or lookup_column '{lookup_column}' does not exist in DataFrame\")\n",
    "    \n",
    "    if extra_details_col is not None and extra_details_col not in df.columns:\n",
    "        raise ValueError(f\"show_supplementary_column '{extra_details_col}' does not exist in DataFrame\")\n",
    "    \n",
    "    if extra_details_col is None:\n",
    "        print(f\"Sampling questions with '{lookup_column}' set to True:\")\n",
    "\n",
    "        sample_list = df[df[lookup_column]].sample(sample_count)[['category', 'question']]\n",
    "        for idx, row in sample_list.iterrows():\n",
    "            print(f\"{idx} - [{row['category']}] {row['question']}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Sampling questions with '{lookup_column}' set to True, showing '{extra_details_col}':\")\n",
    "\n",
    "        sample_list = df[df[lookup_column]].sample(sample_count)[['category', 'question', extra_details_col]]\n",
    "\n",
    "        for idx, row in sample_list.iterrows():\n",
    "            print(f\"{idx} - [{row['category']}] {row['question']} (Extra: {row[extra_details_col]})\")\n",
    "        \n",
    "    \n",
    "\n",
    "    print()\n",
    "\n",
    "def strip_quotes(text):\n",
    "    \"\"\"\n",
    "    Strips quotes only if it appears at the start and end of the argument text.\n",
    "    \"\"\"\n",
    "    if text.startswith('\"') and text.endswith('\"'):\n",
    "        return text[1:-1]\n",
    "    elif text.startswith(\"'\") and text.endswith(\"'\"):\n",
    "        return text[1:-1]\n",
    "    return text\n",
    "\n",
    "def is_valid_roman(string):\n",
    "    \"\"\"\n",
    "    Checks if the input text is a valid Roman numeral.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input string to check.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the text is a valid Roman numeral, False otherwise.\n",
    "    \"\"\"\n",
    "    return bool(ROMAN_NUMERAL_PATTERN.match(string))\n",
    "\n",
    "def is_noun_roman_bigram(bigram):\n",
    "    \"\"\"\n",
    "    Checks if the bigram contains a valid Roman numeral and a noun.\n",
    "\n",
    "    Args:\n",
    "        bigram (tuple): A tuple containing two words (bigram).\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the first word is a valid Roman numeral and the second word is a noun, False otherwise.\n",
    "    \"\"\"\n",
    "    return is_noun(bigram[0]) and is_valid_roman(bigram[1])\n",
    "\n",
    "def find_valid_roman_numerals(input_text, debug=False):\n",
    "    \"\"\"\n",
    "    Parses the input text to find and return valid Roman numerals.\n",
    "\n",
    "    Args:\n",
    "        input_text (str): The input string to parse.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of valid Roman numerals found in the input text.\n",
    "    \"\"\"\n",
    "    # Split the input text into words\n",
    "    # input_tokens = nltk.word_tokenize(input_text)\n",
    "    input_tokens = input_text.split()\n",
    "    valid_roman_numerals = []\n",
    "\n",
    "    for idx in range(len(input_tokens)):\n",
    "        # Check if the word is a valid Roman numeral\n",
    "        if debug:\n",
    "            print(f\"Checking token: {input_tokens[idx]} at index {idx}\")\n",
    "        \n",
    "        # If entire token is not uppercase, skip it\n",
    "        if not input_tokens[idx].isupper():\n",
    "            continue\n",
    "\n",
    "        if len(input_tokens[idx]) <= 2:\n",
    "            if idx == 0:\n",
    "                continue  # Short words at the start are likely not valid Roman numerals\n",
    "\n",
    "            first_word, second_word = input_tokens[idx-1], input_tokens[idx]\n",
    "            # If first word ends in comma or period, skip it\n",
    "            if first_word.endswith((',', '.', ';', ':')):\n",
    "                if debug:\n",
    "                    print(f\"Skipping due to punctuation: {first_word}\")\n",
    "                continue\n",
    "\n",
    "            # Remove any symbols from first word\n",
    "            first_word = re.sub(r'[^\\w\\s]', '', first_word)\n",
    "\n",
    "            # Check if previous word is a noun followed by a valid Roman numeral\n",
    "            if is_noun_roman_bigram((first_word, second_word)):\n",
    "                # If the previous word is a noun and the current word is a valid Roman numeral\n",
    "                if debug:\n",
    "                    print(f\"Found valid Roman numeral: {first_word} {second_word} -- adding {input_tokens[idx]}\")\n",
    "                valid_roman_numerals.append(input_tokens[idx])\n",
    "        else: \n",
    "            if is_valid_roman(input_tokens[idx]):\n",
    "                valid_roman_numerals.append(input_tokens[idx])\n",
    "\n",
    "    if debug:\n",
    "        print(f\"Original input: {input_text}\")\n",
    "        print(f\"Valid Roman numerals found: {valid_roman_numerals}\")\n",
    "\n",
    "    \n",
    "    # Filter and return only valid Roman numerals\n",
    "    return valid_roman_numerals\n",
    "\n",
    "def is_noun(word):\n",
    "    # NLTK POS tagger expects a list of tokens\n",
    "    pos = nltk.pos_tag([word])[0][1]\n",
    "    # Nouns in Penn Treebank tagset start with 'NN'\n",
    "    return pos.startswith('NN')\n",
    "\n",
    "\n",
    "# def find_non_english_word(input_text :str, method=\"wordnet\", stopword_list=STOPWORDS, lemmatizer=lemmatizer, debug=False) -> list[str]:\n",
    "def find_non_english_word(\n",
    "        input_text: str,\n",
    "        method: str = \"wordnet\",\n",
    "        stopword_list: set[str] | list[str] = set(),\n",
    "        lemmatizer: \"WordNetLemmatizer\" = lemmatizer,\n",
    "        debug: bool = False\n",
    "    ) -> list[str]:\n",
    "\n",
    "    \"\"\"\n",
    "    Identify non-English words in the input text using POS tagging and lemmatization.\n",
    "\n",
    "    Args:\n",
    "        input_text (str): The input string to analyze.\n",
    "        method (str, optional): Method to determine if a word is English.\n",
    "            - \"wordnet\": Uses WordNet synsets (default).\n",
    "            - \"en_dict\": Uses the NLTK English vocabulary word list.\n",
    "        stopword_list (set or list of str): Required. A set or list of stopwords to ignore (e.g., set(stopwords.words(\"english\"))).\n",
    "        lemmatizer (WordNetLemmatizer): Required. An instance of WordNetLemmatizer must be provided by the caller.\n",
    "        debug (bool, optional): If True, prints debug information. Default is False.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of non-English words found in the input text.\n",
    "\n",
    "    Notes:\n",
    "        - Only alphabetic, non-stopword, and non-function-tagged words are checked.\n",
    "        - POS tags excluded: NNP, NNPS, IN, DT, WP, WP$, WRB, PRP, PRP$, CC, TO, MD, EX, UH.\n",
    "        - The method parameter controls the English word check:\n",
    "            * \"wordnet\": A word is considered English if it has WordNet synsets.\n",
    "            * \"en_dict\": A word is considered English if it is in the NLTK English vocabulary.\n",
    "    \"\"\"\n",
    "    # Tags only used for tagged-based filtering\n",
    "    excluded_pos_tags = ['NNP', 'NNPS', 'IN', 'DT', 'WP', 'WP$', 'WRB', 'PRP', 'PRP$', 'CC', 'TO', 'MD', 'EX', 'UH']\n",
    "\n",
    "    # Tokenize and POS tag the full sentence\n",
    "    input_tokens = nltk.word_tokenize(input_text)\n",
    "    tagged_tokens = nltk.pos_tag(input_tokens)\n",
    "\n",
    "    # Filter out tokens that are not alphabetic or are stopwords or in one of the predefined function tags\n",
    "    filtered_tokens = [\n",
    "        (word, tag) for word, tag in tagged_tokens\n",
    "        if word.isalpha() and word.lower() not in stopword_list and tag not in excluded_pos_tags\n",
    "    ]\n",
    "\n",
    "    non_english_words = []\n",
    "    for word, tag in filtered_tokens:\n",
    "        lemma_word = lemmatizer.lemmatize(word.lower())\n",
    "        if method == \"wordnet\" and not wordnet.synsets(lemma_word):\n",
    "            # Check lemma words in WordNet only\n",
    "            non_english_words.append(word)\n",
    "        elif method == \"en_dict\" and not (lemma_word in ENGLISH_VOCAB or lemma_word.lower() in ENGLISH_VOCAB or lemma_word.upper() in ENGLISH_VOCAB or word.lower() in ENGLISH_VOCAB):\n",
    "            # Check lemma words in the NLTK English vocabulary\n",
    "            non_english_words.append(word)\n",
    "        elif method == \"combined\":\n",
    "            # Check both WordNet and NLTK English vocabulary\n",
    "            in_wordnet = bool(wordnet.synsets(lemma_word))\n",
    "            in_en_dict = lemma_word in ENGLISH_VOCAB or lemma_word.lower() in ENGLISH_VOCAB or lemma_word.upper() in ENGLISH_VOCAB or word.lower() in ENGLISH_VOCAB\n",
    "            if not (in_wordnet or in_en_dict):\n",
    "                non_english_words.append(word)\n",
    "\n",
    "        if debug:\n",
    "            print(f\"Word: {word}, Tag: {tag}, Lemma: {lemma_word}, In WordNet: {bool(wordnet.synsets(lemma_word))}, In English Dict: {lemma_word in ENGLISH_VOCAB or lemma_word.lower() in ENGLISH_VOCAB or lemma_word.upper() in ENGLISH_VOCAB or word.lower() in ENGLISH_VOCAB}\")\n",
    "\n",
    "    if debug:\n",
    "        print(f\"Original input: {input_text}\")\n",
    "        print(f\"Non-English words found: {non_english_words}\")\n",
    "\n",
    "    return non_english_words\n",
    "\n",
    "\n",
    "def find_proper_nouns(text: str, debug: str = False) -> list[str]:\n",
    "    \"\"\"\n",
    "    Extract all proper nouns (NNP, NNPS) from the input text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input string to analyze.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: A list of proper nouns found in the text.\n",
    "    \"\"\"\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "\n",
    "    if debug:\n",
    "        print(f\"Input text: {text}\")\n",
    "        for word, tag in tagged:\n",
    "            print(f\"Word: {word}, Tag: {tag}\")\n",
    "\n",
    "    return [word for word, tag in tagged if tag in (\"NNP\", \"NNPS\")]\n",
    "\n",
    "def generate_word_frequency(df: pd.DataFrame, column: str):\n",
    "    if column not in df.columns:\n",
    "        raise ValueError(f\"Column '{column}' does not exist in DataFrame\")\n",
    "    \n",
    "    # Concatenate all questions into a single string\n",
    "    all_text = ' '.join(df[column].astype(str).tolist())\n",
    "\n",
    "    # Convert to lowercase and remove HTML tags\n",
    "    all_text = all_text.lower()\n",
    "    all_text = strip_html_tags(all_text)\n",
    "\n",
    "    # Tokenize the text into words\n",
    "    words = nltk.word_tokenize(all_text)\n",
    "    # Create a frequency distribution of the words\n",
    "    freq_dist = nltk.FreqDist(words)\n",
    "\n",
    "    # Convert the frequency distribution to a DataFrame\n",
    "    freq_df = pd.DataFrame(freq_dist.items(), columns=['word', 'frequency'])\n",
    "\n",
    "    # Sort the DataFrame by frequency in descending order\n",
    "    freq_df = freq_df.sort_values(by='frequency', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    return freq_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fdfd8d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Barack', 'Obama', 'President', 'United', 'States']\n"
     ]
    }
   ],
   "source": [
    "# find_non_english_word(\"This soccer player whose real first name is Mariel is one of 4 women to have scored over 100 goals in international play.\", debug=True)\n",
    "\n",
    "# nltk.pos_tag(['Richard'])[0][1] == 'NNP'\n",
    "\n",
    "# lemmatizer.lemmatize('praciticing', 'v')\n",
    "\n",
    "proper_nouns = find_proper_nouns(\"Barack Obama was the 44th President of the United States.\")\n",
    "print(proper_nouns)  # Output: ['Barack', 'Obama', 'United', 'States']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "152d9fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the data\n",
    "JSON_FILE_PATH = './dataset/JEOPARDY_QUESTIONS1.json'\n",
    "df = load_json_data(JSON_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f66002ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-cleaning steps\n",
    "\n",
    "# create copy of question to original_question\n",
    "df['original_question'] = df['question'].copy()\n",
    "\n",
    "# Strip HTML tags from the 'question' column\n",
    "df['question'] = df['question'].apply(strip_html_tags)\n",
    "\n",
    "# Strip quotes from the 'question' column\n",
    "df['question'] = df['question'].apply(strip_quotes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5fab89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for spelled numbers in the 'question' column\n",
    "df['has_spelled_number'] = df['question'].apply(\n",
    "    lambda x: bool(set(nltk.word_tokenize(x.lower())) & SPELLED_NUMBERS_SET)\n",
    ")\n",
    "\n",
    "# Extract roman numerals in the 'question' column\n",
    "df['roman_text'] = df['question'].apply(\n",
    "    lambda x: find_valid_roman_numerals(x)\n",
    ")\n",
    "df['has_roman_numeral'] = df['roman_text'].apply(\n",
    "    lambda x: len(x) > 0\n",
    ")\n",
    "\n",
    "# Check for numerical values in the 'question' column\n",
    "df['has_numerical_value'] = df['question'].apply(\n",
    "    lambda x: any(re.search(r'\\d', token) for token in nltk.word_tokenize(x))\n",
    ")\n",
    "\n",
    "# has_number \n",
    "df['has_number'] = df['has_spelled_number'] | df['has_numerical_value'] | df['has_roman_numeral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5d96a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling questions with 'has_spelled_number' set to True:\n",
      "44099 - [\"PEACE\", BRO] The first  volunteers in this U.S. government humanitarian force went to Ghana & Tanzania in 1961\n",
      "19452 - [LEX' LAB] Lex traded in his optical one for a scanning electron one & increased his magnification 50x\n",
      "167080 - [CINEMA TRIVIA] Harrison Ford famously shot a swordsman in this first Indiana Jones film because he was too ill to do a fight scene\n",
      "\n",
      "Sampling questions with 'has_roman_numeral' set to True:\n",
      "17419 - [DYNASTY] The House of Savoy-Carignano was shuttered up in this country in 1946 when Humbert II left the throne\n",
      "77487 - [FOOD & DRINK] Antonin Careme created Charlotte Russe & this sour-creamed meat dish for Czar Alexander I\n",
      "8413 - [SHAKESPEAREAN WORDS] This word in \"Henry VI Part 2\" meant blase & world-weary, not having to do with nephrite\n",
      "\n",
      "Sampling questions with 'has_numerical_value' set to True:\n",
      "66788 - [STAR LIGHT, STAR BRIGHT] A spectral class G yellow star, it has an apparent magnitude of about -26\n",
      "18623 - [RANKS & TITLES] From 1867 to 1914 this African country was ruled by a Turkish viceroy called a khedive\n",
      "203838 - [WHEN THEY WERE KIDS] By the age of 5 this 18th century child prodigy known as Wolferl was already composing music\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Output samples of filtered numbers\n",
    "display_sample_questions(df, 'has_spelled_number')\n",
    "display_sample_questions(df, 'has_roman_numeral')\n",
    "display_sample_questions(df, 'has_numerical_value')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486ac829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing questions for non-English words... (this may take a while) Done.\n"
     ]
    }
   ],
   "source": [
    "print(\"Parsing questions for non-English words (this may take few minutes)...\", end=' ')\n",
    "\n",
    "df['non_english_words'] = df['question'].apply(\n",
    "    lambda x: find_non_english_word(x, method=\"combined\", stopword_list=STOPWORDS, lemmatizer=lemmatizer, debug=False)\n",
    ")\n",
    "\n",
    "print(\"Parsing finished.\", end='\\n')\n",
    "\n",
    "df['has_non_english_word'] = df['non_english_words'].apply(\n",
    "    lambda x: len(x) > 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb3d0ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling questions with 'has_non_english_word' set to True, showing 'non_english_words':\n",
      "86846 - [THE 1870s] Due to growing responsibilities of the Attorney General, Congress established this Cabinet dept. on June 22, 1870 (Extra: ['dept'])\n",
      "101649 - [WHERE AM I?] (Cheryl of the Clue Crew will stands in front of a building that extends partway over a river.)  I'm at the library of this president who says it symbolizes his efforts to build bridges from yesterday to tomorrow (Extra: ['partway'])\n",
      "183490 - [SITCOMS] Jon Cryer plays an agent who moved up from mailroom clerk at the Unlimited Talent Agency in this CBS sitcom (Extra: ['mailroom'])\n",
      "29587 - [FLOWERS & CANDY] (Kelly of the Clue Crew shows some candy at the See's Candy Factory.)  A cookbook from the days of the Roman Empire has a recipe for \"nucatum\" which is the Latin root of this confection still popular today (Extra: ['nucatum'])\n",
      "155063 - [TV CATCHPHRASES] \"What you talkin' 'bout, Willis?\" (Extra: ['talkin'])\n",
      "208894 - [ONE FACT AMONG THE FALSE GOSSIP] This dame is really from Des Moines!  Between 1998 & 2007, this dame got 6 Oscar noms!  This dame is having Kid Rock's love child! (Extra: ['noms'])\n",
      "173268 - [TREES] The \"biloba\" in its scientific name means \"2-lobed\", referring to its 2-lobed leaves (Extra: ['biloba'])\n",
      "105492 - [FOREIGN WORDS & PHRASES] Meaning \"double point\", the German doppelpunkt refers to this mark of punctuation (Extra: ['doppelpunkt'])\n",
      "199213 - [LANGUAGES & DIALECTS] The official languages of this small African kingdom are siSwati & English (Extra: ['siSwati'])\n",
      "130488 - [INTERNATIONAL COOKING] Souvlakia is this country's equivalent of shishkebab (Extra: ['shishkebab'])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show sample questions with non-English words\n",
    "display_sample_questions(df, lookup_column='has_non_english_word', extra_details_col='non_english_words', sample_count=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bd390de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test if 'richard' is nnp\n",
    "# print(\"others\" in ENGLISH_VOCAB)\n",
    "# # nltk.pos_tag(['Run'])[0][1] == 'NNP'\n",
    "\n",
    "# find_non_english_word(df.iloc[175864].question, method=\"en_dict\", stopword_list=STOPWORDS, lemmatizer=lemmatizer, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8fa6ef97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unusual Proper Nouns\n",
    "\n",
    "df['proper_nouns'] = df['question'].apply(\n",
    "    lambda x: find_proper_nouns(x, debug=False)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6cdfd59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate word frequency DataFrame for the 'question' column\n",
    "frequency_df = generate_word_frequency(df, 'question')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d731d9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter frequency to 1 and extract all the words\n",
    "unusual_words = set(frequency_df[frequency_df['frequency'] <= LOW_FREQUENCY_THRESHOLD]['word'].tolist())\n",
    "\n",
    "df['has_unusual_proper_noun'] = df['proper_nouns'].apply(\n",
    "    lambda x: bool(set(word.lower() for word in x) & unusual_words)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1a36ac3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "has_unusual_proper_noun\n",
       "False    189531\n",
       "True      27399\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count the number of has_unusual_proper_noun\n",
    "df['has_unusual_proper_noun'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cd1e125e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling questions with 'has_unusual_proper_noun' set to True, showing 'proper_nouns':\n",
      "63367 - [DON'T LISTEN TO THEM!] In 1839 French painter Paul Delaroche said, \"From today painting is dead\" after an exhibition of these (Extra: ['Paul', 'Delaroche'])\n",
      "143857 - [A MATTER OF TASTE] Despite the name of this brand of spread, chef Anthony Bourdain says, \"I can\" (Extra: ['Anthony', 'Bourdain'])\n",
      "19053 - [GOOD OLD SOUTHERN EATS] A popular recipe for this pie was created by the wife of a sales executive at Karo syrup (Extra: ['Karo'])\n",
      "117017 - [ROLLING STONE'S 20 MOST ANNOYING SONGS] This Sisqo song about a piece of clothing mentioned another song on the list, \"Livin' La Vida Loca\" (Extra: ['Sisqo', 'Livin', 'La', 'Vida', 'Loca'])\n",
      "204818 - [SCOTLAND] This Scottish seaport is the seat of the Strathclyde state (Extra: ['Strathclyde'])\n",
      "99261 - [HAPPY 200th, LINCOLN & DARWIN] Born in Kentucky Feb.12, 1809, Abe Lincoln lived from age 7 to 21 in this state before the move to Illinois (Extra: ['Kentucky', 'Feb.12', 'Abe', 'Lincoln', 'Illinois'])\n",
      "211912 - [NOTORIOUS] Frank Galluccio gave him the scar that earned him the nickname \"Scarface\" (Extra: ['Frank', 'Galluccio', 'Scarface'])\n",
      "215262 - [A NOVEL CATEGORY] Elias Canetti's \"Die Blendung\" has been published in English under the Biblical title \"The Tower of\" this (Extra: ['Elias', 'Canetti', 'Die', 'Blendung', 'English', 'Biblical', 'Tower'])\n",
      "89460 - [MISCELLANY] In Latin this Charlie Rich song would be titled \"Januis Clausis\" (Extra: ['Latin', 'Charlie', 'Rich', 'Januis', 'Clausis'])\n",
      "46154 - [INTERNATIONAL BOOKS & AUTHORS] Former first lady Jehan Sadat called her moving memoir \"A Woman of\" this country (Extra: ['Former', 'Jehan', 'Sadat', 'A', 'Woman'])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_sample_questions(df, lookup_column='has_unusual_proper_noun', extra_details_col='proper_nouns', sample_count=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
