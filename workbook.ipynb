{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1eba6900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import logging\n",
    "\n",
    "from nltk.corpus import wordnet, words, stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger_eng', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('words', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694759cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants and Patterns\n",
    "\n",
    "# Regex for matching roman numerals\n",
    "ROMAN_NUMERAL_PATTERN = re.compile(\n",
    "    r'^(M{0,3})(CM|CD|D?C{0,3})'\n",
    "    r'(XC|XL|L?X{0,3})(IX|IV|V?I{0,3})$', re.IGNORECASE\n",
    ")\n",
    "\n",
    "# Spelled-out numbers\n",
    "SPELLED_NUMBERS = [\n",
    "    \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\",\n",
    "    \"eleven\", \"twelve\", \"thirteen\", \"fourteen\", \"fifteen\", \"sixteen\", \"seventeen\", \"eighteen\", \"nineteen\", \"twenty\",\n",
    "    \"thirty\", \"forty\", \"fifty\", \"sixty\", \"seventy\", \"eighty\", \"ninety\",\n",
    "    \"hundred\", \"thousand\", \"million\", \"billion\"\n",
    "]\n",
    "\n",
    "SPELLED_NUMBERS += ['first', 'second', 'third', 'fourth', 'fifth', 'sixth', 'seventh', 'eighth', 'ninth', 'tenth']\n",
    "SPELLED_NUMBERS += ['eleventh', 'twelfth', 'thirteenth', 'fourteenth', 'fifteenth', 'sixteenth', 'seventeenth', 'eighteenth', 'nineteenth']\n",
    "SPELLED_NUMBERS += ['twentieth', 'thirtieth', 'fortieth', 'fiftieth', 'sixtieth', 'seventieth', 'eightieth', 'ninetieth']\n",
    "SPELLED_NUMBERS += ['hundredth', 'thousandth', 'millionth', 'billionth']\n",
    "\n",
    "SPELLED_NUMBERS += ['twice', 'thrice', 'once']\n",
    "SPELLED_NUMBERS += ['single', 'double', 'triple', 'quadruple', 'quintuple', 'sextuple', 'septuple', 'octuple', 'nonuple', 'decuple']\n",
    "SPELLED_NUMBERS += ['dozen', 'fortnight', 'score', 'century', 'millennium']\n",
    "\n",
    "STOPWORDS = set(stopwords.words(\"english\"))\n",
    "\n",
    "ENGLISH_VOCAB = set(w.lower() for w in words.words())\n",
    "\n",
    "NUMBER_OF_SAMPLES_TO_SHOW = 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba9075e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# File functions\n",
    "# Load JSON data from a file\n",
    "def load_json_data(file_path):\n",
    "    '''\n",
    "    Loads JSON data from the specified file path into a pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the JSON file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing the loaded JSON data, or None if loading fails.\n",
    "    '''\n",
    "    try:\n",
    "        data = pd.read_json(file_path, orient=\"records\", lines=False)\n",
    "        return data\n",
    "    except ValueError as e:\n",
    "        print(f\"Error loading JSON data: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def strip_html_tags(text):\n",
    "    \"\"\"\n",
    "    Strips HTML tags from a string.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input string potentially containing HTML tags.\n",
    "\n",
    "    Returns:\n",
    "        str: The string with HTML tags removed.\n",
    "    \"\"\"\n",
    "    # Regex to find any HTML tag: < followed by any characters, then >\n",
    "    # The '?' makes it non-greedy, matching the shortest possible string.\n",
    "    # The '|' handles self-closing tags like <br/> or <img src=\"...\">\n",
    "    # and also comments (though less common in user-generated text usually)\n",
    "    clean = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
    "    return re.sub(clean, '', text)\n",
    "\n",
    "def display_sample_questions(df, lookup_column = None, extra_details_col = None, sample_count = 3):\n",
    "    \"\"\"\n",
    "    Displays a sample of questions from the DataFrame where the specified lookup_column is True.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing the questions.\n",
    "        lookup_column (str, optional): The column name to use as a boolean filter for selecting rows.\n",
    "        sample_count (int, optional): The number of sample questions to display. Default is 3.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If lookup_column is None or empty.\n",
    "\n",
    "    Prints:\n",
    "        The index, category, and question text for each sampled row.\n",
    "    \"\"\"\n",
    "    if lookup_column is None:\n",
    "        raise ValueError(\"lookup_column argument cannot be empty or None\")\n",
    "    \n",
    "    if df is None or lookup_column not in df.columns:\n",
    "        raise ValueError(f\"DataFrame is None or lookup_column '{lookup_column}' does not exist in DataFrame\")\n",
    "    \n",
    "    if extra_details_col is not None and extra_details_col not in df.columns:\n",
    "        raise ValueError(f\"show_supplementary_column '{extra_details_col}' does not exist in DataFrame\")\n",
    "    \n",
    "    if extra_details_col is None:\n",
    "        print(f\"Sampling questions with '{lookup_column}' set to True:\")\n",
    "\n",
    "        sample_list = df[df[lookup_column]].sample(sample_count)[['category', 'question']]\n",
    "        for idx, row in sample_list.iterrows():\n",
    "            print(f\"{idx} - [{row['category']}] {row['question']}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Sampling questions with '{lookup_column}' set to True, showing '{extra_details_col}':\")\n",
    "\n",
    "        sample_list = df[df[lookup_column]].sample(sample_count)[['category', 'question', extra_details_col]]\n",
    "\n",
    "        for idx, row in sample_list.iterrows():\n",
    "            print(f\"{idx} - [{row['category']}] {row['question']} (Extra: {row[extra_details_col]})\")\n",
    "        \n",
    "    \n",
    "\n",
    "    print()\n",
    "\n",
    "def strip_quotes(text):\n",
    "    \"\"\"\n",
    "    Strips quotes only if it appears at the start and end of the argument text.\n",
    "    \"\"\"\n",
    "    if text.startswith('\"') and text.endswith('\"'):\n",
    "        return text[1:-1]\n",
    "    elif text.startswith(\"'\") and text.endswith(\"'\"):\n",
    "        return text[1:-1]\n",
    "    return text\n",
    "\n",
    "def is_valid_roman(string):\n",
    "    \"\"\"\n",
    "    Checks if the input text is a valid Roman numeral.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input string to check.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the text is a valid Roman numeral, False otherwise.\n",
    "    \"\"\"\n",
    "    return bool(ROMAN_NUMERAL_PATTERN.match(string))\n",
    "\n",
    "def is_noun_roman_bigram(bigram):\n",
    "    \"\"\"\n",
    "    Checks if the bigram contains a valid Roman numeral and a noun.\n",
    "\n",
    "    Args:\n",
    "        bigram (tuple): A tuple containing two words (bigram).\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the first word is a valid Roman numeral and the second word is a noun, False otherwise.\n",
    "    \"\"\"\n",
    "    return is_noun(bigram[0]) and is_valid_roman(bigram[1])\n",
    "\n",
    "def find_valid_roman_numerals(input_text, debug=False):\n",
    "    \"\"\"\n",
    "    Parses the input text to find and return valid Roman numerals.\n",
    "\n",
    "    Args:\n",
    "        input_text (str): The input string to parse.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of valid Roman numerals found in the input text.\n",
    "    \"\"\"\n",
    "    # Split the input text into words\n",
    "    # input_tokens = nltk.word_tokenize(input_text)\n",
    "    input_tokens = input_text.split()\n",
    "    valid_roman_numerals = []\n",
    "\n",
    "    for idx in range(len(input_tokens)):\n",
    "        # Check if the word is a valid Roman numeral\n",
    "        if debug:\n",
    "            print(f\"Checking token: {input_tokens[idx]} at index {idx}\")\n",
    "        \n",
    "        # If entire token is not uppercase, skip it\n",
    "        if not input_tokens[idx].isupper():\n",
    "            continue\n",
    "\n",
    "        if len(input_tokens[idx]) <= 2:\n",
    "            if idx == 0:\n",
    "                continue  # Short words at the start are likely not valid Roman numerals\n",
    "\n",
    "            first_word, second_word = input_tokens[idx-1], input_tokens[idx]\n",
    "            # If first word ends in comma or period, skip it\n",
    "            if first_word.endswith((',', '.', ';', ':')):\n",
    "                if debug:\n",
    "                    print(f\"Skipping due to punctuation: {first_word}\")\n",
    "                continue\n",
    "\n",
    "            # Remove any symbols from first word\n",
    "            first_word = re.sub(r'[^\\w\\s]', '', first_word)\n",
    "\n",
    "            # Check if previous word is a noun followed by a valid Roman numeral\n",
    "            if is_noun_roman_bigram((first_word, second_word)):\n",
    "                # If the previous word is a noun and the current word is a valid Roman numeral\n",
    "                if debug:\n",
    "                    print(f\"Found valid Roman numeral: {first_word} {second_word} -- adding {input_tokens[idx]}\")\n",
    "                valid_roman_numerals.append(input_tokens[idx])\n",
    "        else: \n",
    "            if is_valid_roman(input_tokens[idx]):\n",
    "                valid_roman_numerals.append(input_tokens[idx])\n",
    "\n",
    "    if debug:\n",
    "        print(f\"Original input: {input_text}\")\n",
    "        print(f\"Valid Roman numerals found: {valid_roman_numerals}\")\n",
    "\n",
    "    \n",
    "    # Filter and return only valid Roman numerals\n",
    "    return valid_roman_numerals\n",
    "\n",
    "def is_noun(word):\n",
    "    # NLTK POS tagger expects a list of tokens\n",
    "    pos = nltk.pos_tag([word])[0][1]\n",
    "    # Nouns in Penn Treebank tagset start with 'NN'\n",
    "    return pos.startswith('NN')\n",
    "\n",
    "\n",
    "# def find_non_english_word(input_text :str, method=\"wordnet\", stopword_list=STOPWORDS, lemmatizer=lemmatizer, debug=False) -> list[str]:\n",
    "def find_non_english_word(\n",
    "        input_text: str,\n",
    "        method: str = \"wordnet\",\n",
    "        stopword_list: set[str] | list[str] = set(),\n",
    "        lemmatizer: \"WordNetLemmatizer\" = lemmatizer,\n",
    "        debug: bool = False\n",
    "    ) -> list[str]:\n",
    "\n",
    "    \"\"\"\n",
    "    Identify non-English words in the input text using POS tagging and lemmatization.\n",
    "\n",
    "    Args:\n",
    "        input_text (str): The input string to analyze.\n",
    "        method (str, optional): Method to determine if a word is English.\n",
    "            - \"wordnet\": Uses WordNet synsets (default).\n",
    "            - \"en_dict\": Uses the NLTK English vocabulary word list.\n",
    "        stopword_list (set or list of str): Required. A set or list of stopwords to ignore (e.g., set(stopwords.words(\"english\"))).\n",
    "        lemmatizer (WordNetLemmatizer): Required. An instance of WordNetLemmatizer must be provided by the caller.\n",
    "        debug (bool, optional): If True, prints debug information. Default is False.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of non-English words found in the input text.\n",
    "\n",
    "    Notes:\n",
    "        - Only alphabetic, non-stopword, and non-function-tagged words are checked.\n",
    "        - POS tags excluded: NNP, NNPS, IN, DT, WP, WP$, WRB, PRP, PRP$, CC, TO, MD, EX, UH.\n",
    "        - The method parameter controls the English word check:\n",
    "            * \"wordnet\": A word is considered English if it has WordNet synsets.\n",
    "            * \"en_dict\": A word is considered English if it is in the NLTK English vocabulary.\n",
    "    \"\"\"\n",
    "    # Tags only used for tagged-based filtering\n",
    "    excluded_pos_tags = ['NNP', 'NNPS', 'IN', 'DT', 'WP', 'WP$', 'WRB', 'PRP', 'PRP$', 'CC', 'TO', 'MD', 'EX', 'UH']\n",
    "\n",
    "    # Tokenize and POS tag the full sentence\n",
    "    input_tokens = nltk.word_tokenize(input_text)\n",
    "    tagged_tokens = nltk.pos_tag(input_tokens)\n",
    "\n",
    "    # Filter out tokens that are not alphabetic or are stopwords or in one of the predefined function tags\n",
    "    filtered_tokens = [\n",
    "        (word, tag) for word, tag in tagged_tokens\n",
    "        if word.isalpha() and word.lower() not in stopword_list and tag not in excluded_pos_tags\n",
    "    ]\n",
    "\n",
    "    non_english_words = []\n",
    "    for word, tag in filtered_tokens:\n",
    "        lemma_word = lemmatizer.lemmatize(word.lower())\n",
    "        if method == \"wordnet\" and not wordnet.synsets(lemma_word):\n",
    "            # Check lemma words in WordNet only\n",
    "            non_english_words.append(word)\n",
    "        elif method == \"en_dict\" and not (lemma_word in ENGLISH_VOCAB or lemma_word.lower() in ENGLISH_VOCAB or lemma_word.upper() in ENGLISH_VOCAB or word.lower() in ENGLISH_VOCAB):\n",
    "            # Check lemma words in the NLTK English vocabulary\n",
    "            non_english_words.append(word)\n",
    "        elif method == \"combined\":\n",
    "            # Check both WordNet and NLTK English vocabulary\n",
    "            in_wordnet = bool(wordnet.synsets(lemma_word))\n",
    "            in_en_dict = lemma_word in ENGLISH_VOCAB or lemma_word.lower() in ENGLISH_VOCAB or lemma_word.upper() in ENGLISH_VOCAB or word.lower() in ENGLISH_VOCAB\n",
    "            if not (in_wordnet or in_en_dict):\n",
    "                non_english_words.append(word)\n",
    "\n",
    "        if debug:\n",
    "            print(f\"Word: {word}, Tag: {tag}, Lemma: {lemma_word}, In WordNet: {bool(wordnet.synsets(lemma_word))}, In English Dict: {lemma_word in ENGLISH_VOCAB or lemma_word.lower() in ENGLISH_VOCAB or lemma_word.upper() in ENGLISH_VOCAB or word.lower() in ENGLISH_VOCAB}\")\n",
    "\n",
    "    if debug:\n",
    "        print(f\"Original input: {input_text}\")\n",
    "        print(f\"Non-English words found: {non_english_words}\")\n",
    "\n",
    "    return non_english_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdfd8d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: soccer, Tag: NN, Lemma: soccer, In WordNet: True, In English Dict: True\n",
      "Word: player, Tag: NN, Lemma: player, In WordNet: True, In English Dict: True\n",
      "Word: real, Tag: JJ, Lemma: real, In WordNet: True, In English Dict: True\n",
      "Word: first, Tag: JJ, Lemma: first, In WordNet: True, In English Dict: True\n",
      "Word: name, Tag: NN, Lemma: name, In WordNet: True, In English Dict: True\n",
      "Word: is, Tag: VBZ, Lemma: is, In WordNet: True, In English Dict: True\n",
      "Word: is, Tag: VBZ, Lemma: is, In WordNet: True, In English Dict: True\n",
      "Word: one, Tag: CD, Lemma: one, In WordNet: True, In English Dict: True\n",
      "Word: women, Tag: NNS, Lemma: woman, In WordNet: True, In English Dict: True\n",
      "Word: have, Tag: VB, Lemma: have, In WordNet: True, In English Dict: True\n",
      "Word: scored, Tag: VBN, Lemma: scored, In WordNet: True, In English Dict: True\n",
      "Word: goals, Tag: NNS, Lemma: goal, In WordNet: True, In English Dict: True\n",
      "Word: international, Tag: JJ, Lemma: international, In WordNet: True, In English Dict: True\n",
      "Word: play, Tag: NN, Lemma: play, In WordNet: True, In English Dict: True\n",
      "Original input: This soccer player whose real first name is Mariel is one of 4 women to have scored over 100 goals in international play.\n",
      "Non-English words found: []\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_non_english_word(\"This soccer player whose real first name is Mariel is one of 4 women to have scored over 100 goals in international play.\", debug=True)\n",
    "\n",
    "# nltk.pos_tag(['Richard'])[0][1] == 'NNP'\n",
    "\n",
    "# lemmatizer.lemmatize('praciticing', 'v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "152d9fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the data\n",
    "JSON_FILE_PATH = './dataset/JEOPARDY_QUESTIONS1.json'\n",
    "df = load_json_data(JSON_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f66002ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-cleaning steps\n",
    "\n",
    "# create copy of question to original_question\n",
    "df['original_question'] = df['question'].copy()\n",
    "\n",
    "# Strip HTML tags from the 'question' column\n",
    "df['question'] = df['question'].apply(strip_html_tags)\n",
    "\n",
    "# Strip quotes from the 'question' column\n",
    "df['question'] = df['question'].apply(strip_quotes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae5fab89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for spelled numbers in the 'question' column\n",
    "df['has_spelled_number'] = df['question'].apply(\n",
    "    lambda x: any(word in SPELLED_NUMBERS for word in nltk.word_tokenize(x.lower()))\n",
    ")\n",
    "\n",
    "# Extract roman numerals in the 'question' column\n",
    "df['roman_text'] = df['question'].apply(\n",
    "    lambda x: find_valid_roman_numerals(x)\n",
    ")\n",
    "df['has_roman_numeral'] = df['roman_text'].apply(\n",
    "    lambda x: len(x) > 0\n",
    ")\n",
    "\n",
    "# Check for numerical values in the 'question' column\n",
    "df['has_numerical_value'] = df['question'].apply(\n",
    "    lambda x: any(re.search(r'\\d', token) for token in nltk.word_tokenize(x))\n",
    ")\n",
    "\n",
    "# has_number \n",
    "df['has_number'] = df['has_spelled_number'] | df['has_numerical_value'] | df['has_roman_numeral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5d96a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling questions with 'has_spelled_number' set to True:\n",
      "127425 - [AUTHOR-ITATIVE INFORMATION] In 1913 he published his first book of poems as well as \"Sons and Lovers\"\n",
      "214396 - [QUOTATIONS] (Jon of the Clue Crew reports from the Freud Museum in Vienna.) Despite all the research that's been done on Freud, no one can definitely say if, or when, he made the attributed remark \"sometimes a\" this \"is just a\" this\n",
      "78634 - [WELCOME BACK, QATAR] This satellite TV news service based in Qatar has become one of the most important Middle East broadcasters\n",
      "\n",
      "Sampling questions with 'has_roman_numeral' set to True:\n",
      "156466 - [1492] Casimir IV was succeeded as king of this country by his son John Albert\n",
      "128942 - [YOU TAKE A MILE] You might be MIA at MIA, this city's international airport; its concourse D is 1.3 miles long, end to end\n",
      "209451 - [WORLD HISTORY] In 1488 this king of Spain sent 100 Moorish slaves to Pope Innocent VIII who gave them as gifts to Cardinals\n",
      "\n",
      "Sampling questions with 'has_numerical_value' set to True:\n",
      "87615 - [BONE UP ON YOUR BONAPARTE] This royal house was restored to the French throne in 1815 following Napoleon's final defeat\n",
      "138890 - [BREAKFAST CEREALS] Introduced in the 1980s, it's the trademark name for a cereal of \"seven whole grains & sesame\" & little puffs\n",
      "104487 - [TREATIES] In 1691 England guaranteed this country's Catholics certain rights by the Treaty of Limerick\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Output samples of filtered numbers\n",
    "display_sample_questions(df, 'has_spelled_number')\n",
    "display_sample_questions(df, 'has_roman_numeral')\n",
    "display_sample_questions(df, 'has_numerical_value')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486ac829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing questions for non-English words... (this may take a while)\n"
     ]
    }
   ],
   "source": [
    "print(\"Parsing questions for non-English words... (this may take a while)\", end=' ')\n",
    "\n",
    "df['non_english_words'] = df['question'].apply(\n",
    "    lambda x: find_non_english_word(x, method=\"combined\", stopword_list=STOPWORDS, lemmatizer=lemmatizer, debug=False)\n",
    ")\n",
    "\n",
    "print(\"Done.\", end='\\n')\n",
    "\n",
    "df['has_non_english_word'] = df['non_english_words'].apply(\n",
    "    lambda x: len(x) > 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cb3d0ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling questions with 'has_non_english_word' set to True, showing 'non_english_words':\n",
      "174221 - [NO. 1 HITS OF THE '70s] This No. 1 hit by the Bee Gees says, \"What you doin' and you're laying on your back, aah\" (Extra: ['doin'])\n",
      "102332 - [SWORDS] The name of this highly curved, highly deadly sword comes from the Persian shamshir (Extra: ['shamshir'])\n",
      "34270 - [CHRISTMAS CUISINE] At Christmastime you might have a \"burning\" desire to make a buche de noel, a cake shaped like this (Extra: ['buche'])\n",
      "54287 - [MY OFF-SHOW WARDROBE] Hey, there, Daddy-O--it's the rhymin' name of the threads I'm stylin' here (Extra: ['rhymin', 'stylin'])\n",
      "82097 - [\"RED\" SCIENCE] This salad ingredient, capsicum annuum grossum, has been allowed to ripen on the vine (Extra: ['annuum', 'grossum'])\n",
      "183869 - [SHE INVENTED WHAT?] Rachel Brown & Elizabeth Hazen invented nystatin, an anitbiotic used on people & on this \"Dutch\" tree disease (Extra: ['anitbiotic'])\n",
      "71354 - [BRITCOMS] This 1980-84 britcom about a struggling MP later added \"Prime\" to the title (Extra: ['britcom'])\n",
      "144253 - [WORD PUZZLES] You'd better tell the captain this has happened:----------------------------------MANBOARD (Extra: ['MANBOARD'])\n",
      "62388 - [O BROTHER, WHERE ART THOU?] Huzzah!  Thou hast undertaken a tour of Iqaluit & Yellowknife, territorial capitals in this country (Extra: ['Huzzah', 'hast'])\n",
      "211817 - [RHYME TIME] Rae Dawn Chong starred in this 1984  film about urban kids who breakdance & rap (Extra: ['breakdance'])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show sample questions with non-English words\n",
    "display_sample_questions(df, lookup_column='has_non_english_word', extra_details_col='non_english_words', sample_count=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1586faa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "has_non_english_word\n",
       "False    204598\n",
       "True      12332\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.has_non_english_word.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6209b6",
   "metadata": {},
   "source": [
    "dictionary\n",
    "has_non_english_word -- en_dict limited checks\n",
    "False    118159\n",
    "True      98771\n",
    "Name: count, dtype: int64\n",
    "\n",
    "tagg\n",
    "has_non_english_word using tagg\n",
    "False    198435\n",
    "True      18495\n",
    "Name: count, dtype: int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd390de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Word: testing, Tag: VBG, Lemma: testing, In WordNet: True, In English Dict: True\n",
      "Word: new, Tag: JJ, Lemma: new, In WordNet: True, In English Dict: True\n",
      "Word: drugs, Tag: NNS, Lemma: drug, In WordNet: True, In English Dict: True\n",
      "Word: control, Tag: NN, Lemma: control, In WordNet: True, In English Dict: True\n",
      "Word: group, Tag: NN, Lemma: group, In WordNet: True, In English Dict: True\n",
      "Word: given, Tag: VBN, Lemma: given, In WordNet: True, In English Dict: True\n",
      "Word: nonmedical, Tag: JJ, Lemma: nonmedical, In WordNet: False, In English Dict: True\n",
      "Word: substance, Tag: NN, Lemma: substance, In WordNet: True, In English Dict: True\n",
      "Word: made, Tag: VBN, Lemma: made, In WordNet: True, In English Dict: True\n",
      "Word: sugar, Tag: NN, Lemma: sugar, In WordNet: True, In English Dict: True\n",
      "Word: saline, Tag: JJ, Lemma: saline, In WordNet: True, In English Dict: True\n",
      "Word: solution, Tag: NN, Lemma: solution, In WordNet: True, In English Dict: True\n",
      "Original input: When testing new drugs, a control group is given this nonmedical substance  made of sugar or a saline solution\n",
      "Non-English words found: []\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test if 'richard' is nnp\n",
    "print(\"others\" in ENGLISH_VOCAB)\n",
    "# nltk.pos_tag(['Run'])[0][1] == 'NNP'\n",
    "\n",
    "find_non_english_word(df.iloc[175864].question, method=\"en_dict\", stopword_list=STOPWORDS, lemmatizer=lemmatizer, debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
